from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import os
from etl_normalization_to_db import normalize_and_load

# =====================================================
# Airflow DAG for ETL Normalization Pipeline
# =====================================================

default_args = {
    "owner": "teryaq",
    "depends_on_past": False,
    "email_on_failure": False,
    "email_on_retry": False,
    "retries": 1,
    "retry_delay": timedelta(minutes=5),
}

with DAG(
    "etl_normalization_pipeline",
    default_args=default_args,
    description="ETL pipeline to normalize hospital data and load into staging",
    schedule_interval="@daily",   # run daily
    start_date=datetime(2025, 1, 1),
    catchup=False,
    tags=["etl", "normalization"],
) as dag:

    def process_files():
        data_dir = "/opt/airflow/data"
        for fname in os.listdir(data_dir):
            if fname.endswith((".csv", ".xlsx", ".xls", ".json")):
                hospital_name = fname.split("_")[0]  # e.g. hospitalA_sample.csv â†’ hospitalA
                fpath = os.path.join(data_dir, fname)
                print(f"[DAG] Processing {fpath} for hospital {hospital_name}")
                normalize_and_load(fpath, hospital_name)

    etl_task = PythonOperator(
        task_id="normalize_and_load_files",
        python_callable=process_files,
    )
